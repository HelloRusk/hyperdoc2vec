citing	cited	left context	right context
P18-1222	L08-1291	"We use three datasets from the academic paper domain, i.e., NIPS4, ACL anthology5 and DBLP6, as shown in Table 3. They all contain full text of papers, and are of small, medium, and large size, respectively. We apply ParsCit"	"to parse the citations and bibliography sections. Each identified citation string referring to a paper in the same dataset, e.g., [1] or (Author et al., 2018), is replaced by a global paper id. Consecutive citations like [1, 2] are regarded as multiple ground truths occupying one position. Following He et al. (2010), we take 50 words before and after a citation as the citation context."
P18-1222	D07-1074	"It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can benefit from sophisticated techniques such as collective linking"	"We introduce notations and definitions, then formally define the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness."
P18-1222	N15-1184	"Note that combining the citation and content objectives leads to a joint learning framework. To facilitate easier and faster training, we adopt an alternative pre-training/fine-tuning or retrofitting framework"	"We initialize with a predefined number of pv-dm iterations, and then optimize Eq. 1 based on the initialization."
P18-1222	P13-2006	"Embedding-based entity linking is another topic that exploits embeddings to model certain hyperdocs, i.e., Wikipedia (Huang et al., 2015a; Yamada et al., 2016; Sun et al., 2015; Fang et al., 2016;"	"Zwicklbauer et al., 2016), for entity linking (Shen et al., 2015). It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can benefit from sophisticated techniques such as collective linking (Cucerzan, 2007)."
P18-1222	P07-2045	"To better investigate the impact of context intent awareness, Table 9 shows recommended papers of the running example of this paper. Here, Zhao and Gildea (2010) cited the BLEU metric (Papineni et al., 2002) and Moses tools"	"of machine translation. However, the additional words “machine translation” lead both w2v and d2v-cac to recommend many machine translation papers. Only our h-d2v manages to recognize the citation function “using tools/algorithms (PBas)”, and concentrates on the citation intent to return the right papers in top-5 results."
P18-1222	W06-1613	"In this section, we analyze the impact of context intent awareness. We use"	")’s 2,824 citation contexts11 with annotated citation functions, e.g., emphasizing weakness (Weak) or using tools/algorithms (PBas) of the cited papers. Table 8 from Teufel et al. (2006) describes the full annotating scheme. Teufel et al. (2006) also use manual features to evaluate citation function clas- sification."
P18-1222	W06-1613	"Figure 4 depicts the F1 scores. Scores of"	"’s approach are from the original pa- per. We omit d2v-nc because it is very inferior to d2v-cac. We have the following observations. First, Teufel et al. (2006)’s feature-engineering- based approach has the best performance. Note that we cannot obtain their original cross valida- tion split, so the comparison may not be fair and is only for consideration in terms of numbers."
P18-1222	K16-1025	"The second type designs sophisticated em- bedding models to fulfill certain tasks, e.g., cita- tion recommendation (Huang et al., 2015b), pa- per classification (Wang et al., 2016), and entity linking"	", etc. These models are limited to specific tasks, and it is yet unknown whether embeddings learned for those particular tasks can generalize to others. Based on the above facts, we are interested in two questions:"
P18-1222	D10-1058	"To better investigate the impact of context intent awareness, Table 9 shows recommended papers of the running example of this paper. Here,"	"cited the BLEU metric (Papineni et al., 2002) and Moses tools (Koehn et al., 2007) of machine translation. However, the additional words “machine translation” lead both w2v and d2v-cac to recommend many machine transla- tion papers. Only"
D11-1072	E06-1002	"Recognizing named entities (NER tagging) in natural language text has been extensively addressed in NLP research. The output is labeled noun phrases. However, these are not yet canonical entities, explicitly and uniquely denoted in a knowledge repository. Approaches that use Wikipedia for explicit disambiguation date back to"	"and have been further pursued by (Cucerzan07; Han09; Milne08; Nguyen08; Mihalcea07). (Bunescu06) defined a similarity measure that compared the context of a mention to the Wikipedia categories of an entity candidate."
Q18-1028	P14-5010	"To overcome the class imbalance, we use SMOTE (Chawla et al., 2002) to generate synthetic examples in the training fold using the 5 nearest neighbors. The classifier is implemented using SciKit (Pedregosa et al., 2011) and syntactic processing was done using CoreNLP"	" Selectional preferences used pretrained 300-dimensional GloVe vectors from the 840B token Common Crawl (Pennington et al., 2014). The topic model features used an LDA with 100 topics."
2020.lrec-1.225	N16-2002	"In experiments, we train 600 word embeddings while varying hyperparameters, including training algorithm, corpus, dimension size, and context window. We then measure the accuracies on the BATS dataset"	"for intrinsic evaluation and on the VecEval (Nayak et al., 2016) and SentEval (Conneau and Kiela, 2018) datasets for extrinsic evaluation. The experimental results reveal several salient relationships between BATS and VecEval/SentEval, the results also demonstrate, however, that the dataset for examining inflectional morphology shows correlation disagreement in the evaluation results."